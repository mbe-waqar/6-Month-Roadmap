# Week 1 â€” Advanced Transformers (Day-by-Day Plan & Resources)

This week focuses on understanding **Transformers deeply**, both conceptually and practically, and implementing a **working encoderâ€“decoder Transformer from scratch in PyTorch**.

---

## ğŸŸ¢ Day 1 â€” Transformer Overview & Attention Intuition

### ğŸ¯ Goal

Understand why Transformers were introduced and build intuition for attention.

### âœ… Tasks

- Study limitations of RNNs and CNNs
- Learn high-level idea of self-attention
- Understand encoder vs decoder roles
- Build intuition before math

### ğŸ“˜ What to Study

- Why parallelization matters
- What attention solves in sequence modeling
- Difference between encoder-only and decoder-only models

### ğŸ¥ YouTube

- â€œThe Illustrated Transformerâ€ â€“ Jay Alammar
- â€œAttention Mechanism Explainedâ€ â€“ :contentReference[oaicite:0]{index=0}

### ğŸ“„ Written Resources

- Jay Alammar â€” *The Illustrated Transformer*
- Hugging Face â€” *Transformer Conceptual Guide*

### ğŸ§ª Mini-Assignment

- Write a 1-page explanation of attention in simple words

### â±ï¸ Time

| Task                | Time           |
| ------------------- | -------------- |
| Videos & reading    | 2h             |
| Notes & explanation | 1.5h           |
| **Total**     | **3.5h** |

---

## ğŸŸ¢ Day 2 â€” Self-Attention Mathematics

### ğŸ¯ Goal

Understand the math behind self-attention step by step.

### âœ… Tasks

- Learn Q, K, V vectors
- Compute dot-product attention
- Understand scaling by âˆšdâ‚–
- Apply softmax manually

### ğŸ“˜ What to Study

- Dot product similarity
- Softmax as probability distribution
- Why scaling stabilizes training

### ğŸ¥ YouTube

- â€œSelf-Attention Clearly Explainedâ€ â€“ :contentReference[oaicite:1]{index=1}
- â€œAttention Is All You Need â€“ Explainedâ€ â€“ :contentReference[oaicite:2]{index=2}

### ğŸ“„ Written Resources

- *Attention Is All You Need* â€” Section 3.1
- Harvard NLP â€” *Annotated Transformer*

### ğŸ§ª Mini-Assignment

- Manually compute attention for 3 tokens with 2-D vectors

### â±ï¸ Time

| Task               | Time         |
| ------------------ | ------------ |
| Theory & videos    | 2h           |
| Manual calculation | 2h           |
| **Total**    | **4h** |

---

## ğŸŸ¢ Day 3 â€” Multi-Head Attention & Positional Encoding

### ğŸ¯ Goal

Understand how Transformers scale attention and encode word order.

### âœ… Tasks

- Learn motivation behind multi-head attention
- Study sinusoidal positional encoding
- Compare absolute vs relative positions
- Implement positional encoding in PyTorch

### ğŸ“˜ What to Study

- Why multiple attention heads help
- How sine/cosine encode position
- Generalization to longer sequences

### ğŸ¥ YouTube

- â€œMulti-Head Attention Explainedâ€ â€“ AI Coffee Break
- â€œPositional Encoding Intuitionâ€ â€“ CodeEmporium

### ğŸ“„ Written Resources

- *Attention Is All You Need* â€” Section 3.2
- Harvard NLP â€” Positional Encoding Notes

### ğŸ§ª Mini-Assignment

- Implement positional encoding
- Plot first 50 positions using matplotlib

### â±ï¸ Time

| Task                   | Time         |
| ---------------------- | ------------ |
| Study & videos         | 2h           |
| Coding & visualization | 2h           |
| **Total**        | **4h** |

---

## ğŸŸ¢ Day 4 â€” Encoder & Decoder Architecture

### ğŸ¯ Goal

Understand the full Transformer encoderâ€“decoder architecture.

### âœ… Tasks

- Study encoder block structure
- Study decoder block structure
- Learn masked self-attention
- Understand cross-attention

### ğŸ“˜ What to Study

- Residual connections
- Layer normalization
- Why masking prevents cheating

### ğŸ¥ YouTube

- â€œTransformer Encoder Decoder Explainedâ€ â€“ AI Coffee Break
- â€œMasked Self-Attentionâ€ â€“ CodeEmporium

### ğŸ“„ Written Resources

- *Attention Is All You Need* â€” Section 3
- PyTorch Transformer Documentation

### ğŸ§ª Mini-Assignment

- Draw encoder and decoder blocks with labels

### â±ï¸ Time

| Task             | Time         |
| ---------------- | ------------ |
| Study & videos   | 3h           |
| Diagrams & notes | 1h           |
| **Total**  | **4h** |

---

## ğŸŸ¢ Day 5 â€” Implement Transformer Components

### ğŸ¯ Goal

Implement core Transformer components from scratch.

### âœ… Tasks

- Implement self-attention layer
- Implement multi-head attention
- Implement feed-forward network
- Add residuals and layer norm

### ğŸ“˜ What to Study

- Tensor shapes and broadcasting
- Modular PyTorch design
- Debugging attention layers

### ğŸ¥ YouTube

- â€œImplement Transformer from Scratchâ€ â€“ :contentReference[oaicite:3]{index=3}
- â€œAnnotated Transformer Walkthroughâ€ â€“ Harvard NLP

### ğŸ“„ Written Resources

- Harvard NLP â€” *Annotated Transformer (Code)*
- PyTorch `nn.Module` documentation

### ğŸ§ª Mini-Assignment

- Unit test attention shapes
- Visualize attention matrices

### â±ï¸ Time

| Task            | Time         |
| --------------- | ------------ |
| Coding          | 4h           |
| Debugging       | 1h           |
| **Total** | **5h** |

---

## ğŸŸ¢ Day 6 â€” Build Full Encoder-Decoder + Training

### ğŸ¯ Goal

Train a working translation Transformer.

### âœ… Tasks

- Assemble encoder + decoder
- Prepare Englishâ€“French toy dataset
- Implement training loop
- Add loss masking

### ğŸ“˜ What to Study

- Teacher forcing
- Cross-entropy with padding mask
- Why BLEU matters

### ğŸ¥ YouTube

- â€œSeq2Seq Transformer Trainingâ€ â€“ :contentReference[oaicite:4]{index=4}
- â€œBLEU Score Explainedâ€ â€“ :contentReference[oaicite:5]{index=5}

### ğŸ“„ Written Resources

- PyTorch Translation Tutorial
- SacrÃ©BLEU Documentation

### ğŸ§ª Mini-Assignment

- Train model until loss decreases
- Translate 10 test sentences

### â±ï¸ Time

| Task                  | Time         |
| --------------------- | ------------ |
| Coding                | 4h           |
| Training & evaluation | 2h           |
| **Total**       | **6h** |

---

## ğŸŸ¢ Day 7 â€” Debugging, Visualization & Documentation

### ğŸ¯ Goal

Polish the project into a portfolio-ready deliverable.

### âœ… Tasks

- Visualize attention weights
- Debug gradient flow
- Refactor and clean code
- Write README and documentation

### ğŸ“˜ What to Study

- Attention heatmap interpretation
- Model explainability
- Documentation best practices

### ğŸ¥ YouTube

- â€œVisualizing Attention Weightsâ€ â€“ CodeEmporium
- â€œDebugging Deep Learning Modelsâ€ â€“ :contentReference[oaicite:6]{index=6}

### ğŸ“„ Written Resources

- Jay Alammar â€” *The Illustrated Transformer*
- GitHub README best practices

### ğŸ§ª Mini-Assignment

- Push final code to GitHub
- Add README with explanations and results

### â±ï¸ Time

| Task                    | Time         |
| ----------------------- | ------------ |
| Visualization           | 2h           |
| Documentation & cleanup | 3h           |
| **Total**         | **5h** |

---

## âœ… Week 1 Outcome

By the end of Week 1:

- Understood attention intuitively and mathematically
- Implemented a Transformer encoderâ€“decoder from scratch
- Trained a working translation model
- Visualized and explained attention
- Built a strong foundation for advanced Transformer variants
